{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ISMRM-MIT-CMR/CMR-image-reconstruction/blob/master/HandsOn_MRI_reconstruction_exercise.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/ISMRM-MIT-CMR/CMR-image-reconstruction/master?urlpath=%2Fdoc%2Ftree%2FHandsOn_MRI_reconstruction_exercise.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FQfPO7E4wTw"
      },
      "source": [
        "# Introduction\n",
        "*by [Thomas Kuestner](www.linkedin.com/in/thomaskuestner)* and *[Kerstin Hammernik](https://www.linkedin.com/in/khammernik/)*\n",
        "\n",
        "In this hands-on, you will get introduced to the world of MRI reconstruction, ranging from conventional reconstruction techniques to machine learning solutions. We start with examining the raw k-space data and coil-sensitivity maps and build the multi-coil forward and adjoint operator. In accelerated imaging, the k-space is undersampled leading to an ill-posed problem. To solve this linear inverse problem, the reconstruction is often regularized. First, we solve the linear and a regularized reconstruction problem conventionally, which allows us to deeply understand where we can later connect to machine learning. These regularizations can also later be learned by neural networks. Machine learning reconstructions thereby differ in their input and targeted application: Image enhancement, direct mapping, physics-based unrolling and distribution-based methods. We will shortly touch upon image enhancement and physics-based unrolling and highlight key factors for development here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "First, we install the dependencies and download the data.\n",
        "\n",
        "You may either work on a brain dataset or a heart dataset. Data should be only used for educational purpose.\n",
        "\n",
        "Brain data were acquired on a 3T Siemens Magnetom Vida at the Institute of Biomedical Imaging, Graz University of Technology, Austria. <br/>\n",
        "Heart data were acquired on a 1.5T Siemens Magnetom Aera at the University Hospital of Tuebingen, Tuebingen, Germany. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMZis3rdoWvf",
        "outputId": "d40da2bc-48f1-46b2-9905-829c424c613d"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install PyWavelets git+https://github.com/midas-tum/medutils.git merlinpy-mri merlinth-mri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbO9aJ7m4kq7",
        "outputId": "9e810a42-1fd7-4052-fa02-a666691b5aaf"
      },
      "outputs": [],
      "source": [
        "# download brain data\n",
        "!wget -O brain_cartesian_2D.h5 \"https://www.dropbox.com/s/hclfv3re91qb1v3/brain_cartesian_2D.h5?dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download heart data\n",
        "!wget -O heart_cartesian_2D.h5 \"https://www.dropbox.com/scl/fi/3jm7n3pu503lm8f79cy1k/heart_cartesian_2D.h5?rlkey=1flz47shxehukel4ttx9yt68x&st=b4cxss3o&dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ezeGwAE3ePu",
        "outputId": "16756c73-57cc-491a-ce69-f506b2b1f08c"
      },
      "outputs": [],
      "source": [
        "# Download ESPIRiT code for coil sensitivity map estimation\n",
        "!git clone https://github.com/mikgroup/espirit-python.git\n",
        "!cp espirit-python/espirit.py ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN6i2t_WSAjw"
      },
      "source": [
        "# Magnetic Resonance Image (MRI) Reconstruction\n",
        "\n",
        "The goal is to recover the clean image $x$, which is obtained by undersampled k-space data $y$ and corrupted by additive Gaussian white noise $n$,\n",
        "$$ y = Ax + n. $$\n",
        "The rawdata $y$ was aquired for multiple receive coils. The linear operator $A$ denotes the mapping from image space to k-space.\n",
        "\n",
        "![MRI Inverse Problem](https://github.com/midas-tum/esmrmb_lmr_2022/blob/main/images/mri_inverse_problem.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIA9scmCZAsd"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "In the first step, we examine the avaiable data regarding their shape and their datatype. Note, that we are dealing with complex-valued data here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNj1r9CqS8Mh",
        "outputId": "2e4dabfa-67dd-4e3c-8128-d7dcd50820b5"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import medutils\n",
        "np.random.seed(1001)\n",
        "\n",
        "datatype = 'heart'  # 'brain' or 'heart'\n",
        "if datatype == 'brain':\n",
        "    # brain data\n",
        "    ds = h5py.File('./brain_cartesian_2D.h5', 'r')\n",
        "elif datatype == 'heart':\n",
        "    # heart data\n",
        "    ds = h5py.File('./heart_cartesian_2D.h5', 'r')\n",
        "kspace = ds['kspace'][()]\n",
        "ds.close()\n",
        "\n",
        "print(f'K-Space:')\n",
        "print(f'dtype={kspace.dtype}')\n",
        "print(f'(nCoils, nFE, nPE)={kspace.shape}')\n",
        "nCoils, nFE, nPE = kspace.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_UitpW8VRdG"
      },
      "source": [
        "We observe that we have 16 (brain) / 10 (heart) coils, the number of frequency encoding (readout) points `nFE` equals 640 (brain) / 352 (heart), and the number of phase encoding steps `nPE` is 330 (brain) / 132 (heart). We will come back to this later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prqq7W7pUt9p"
      },
      "source": [
        "## Data Visualization\n",
        "For data visualization, you are free to use any plotting library such as `matplotlib` or use the provided `medutils` package. The `medutils` package has some useful function for visualization:\n",
        "- `kshow` Process the data in log-space\n",
        "- `imshow` Display the magnitude of the image\n",
        "- `plot_array` Re-arrange the images from a 3D array next to each other.\n",
        "- `ksave` Save k-space\n",
        "- `imsave` Save images\n",
        "\n",
        "We will first visualize the `kspace`. The data was acquired with 16 coils. The vertical direction is the frequency encoding (FE) direction, and the horizontal direction is the phase encoding (PE) direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "CO2e_OvXUwLV",
        "outputId": "23d875db-2816-4810-c926-5b6e4805bc68"
      },
      "outputs": [],
      "source": [
        "if datatype == 'brain':\n",
        "    medutils.visualization.kshow(medutils.visualization.plot_array(kspace, M=2, N=8), title='K-space', figsize=(40,20))  # brain\n",
        "elif datatype == 'heart':\n",
        "    medutils.visualization.kshow(medutils.visualization.plot_array(kspace, M=2, N=5), title='K-space', figsize=(40,20))  # heart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt6WTnNqXq9b"
      },
      "source": [
        "## Transforming k-space to image space\n",
        "Let us now start to transform the `kspace` to images. Therefore, we require the centered 2d inverse Fourier transform. Application of the `ifft2c` to the k-space results in individual coil images.\n",
        "\n",
        "**Task 1: Write the function `ifft2c(kspace)`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "_zD9fi0BVzvi",
        "outputId": "7bf586a2-9185-4f6d-d9c3-8c6ac1613138"
      },
      "outputs": [],
      "source": [
        "def ifft2c(kspace):\n",
        "  # TODO implement the centered inverse FFT.\n",
        "  return _\n",
        "\n",
        "coil_img = ifft2c(kspace)\n",
        "if datatype == 'brain':\n",
        "    medutils.visualization.imshow(medutils.visualization.plot_array(coil_img, M=2, N=8), title='Coil Images', figsize=(40,20))\n",
        "elif datatype == 'heart': \n",
        "    medutils.visualization.imshow(medutils.visualization.plot_array(coil_img, M=2, N=5), title='Coil Images', figsize=(40,20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em5MdDLkYBYy"
      },
      "source": [
        "You might notice several things. First, you see that only a fraction of the image is bright. This is due to the effect that the coils are sensitive only in a certain spatial region. Second, you might notice that there are a lot of black areas all over the image, especially in column direction. This extended field of view in read-out direction, also termed frequency-encoding direction is actually for free, i.e. does not cost any additional acquisition time, and is acquired per default on MRI scanners. Assuming the base resolution is 320, the number of frequency encoding steps is (at least) doubled. This frequency oversampling results in an increased field-of-view in this direction. After the image is transformed to image domain, only the central part needs to be visualized. Thus, for display, we will from now on only consider the central part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kbOdgVwUG1I"
      },
      "source": [
        "## Root-Sum-of-Squares Reconstruction\n",
        "\n",
        "Now, we calculate the Root-Sum-of-Squares reconstruction $x_{rss}$\n",
        "$$ x_{rss} = \\sqrt{ \\sum_{c=1}^{nCoils} \\vert x_c \\vert ^ 2 }, $$\n",
        "\n",
        "where $x_c$ are the individual coil images. Note that using the root-sum-of-squares reconstruction, the phase information of the complex-valued data gets lost. \n",
        "\n",
        "We now visualize only the central part of the reconstructed image of size `[nFE//2, nPE]`. \n",
        "\n",
        "**Task 2: Implement the RSS reconstruction `rss(coil_img)`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "qeYHaNA7VZXt",
        "outputId": "224bacf0-a3ad-4ad7-bb58-75213dcce711"
      },
      "outputs": [],
      "source": [
        "def rss(coil_img):\n",
        "  # TODO implement the rss reconstruction\n",
        "  return _\n",
        "\n",
        "x_rss = rss(coil_img)\n",
        "medutils.visualization.imshow(medutils.visualization.center_crop(x_rss, (nFE//2, nPE)), figsize=(10,10), title='RSS reconstruction')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izcaa68j5-Qa"
      },
      "source": [
        "# Sensitivity Map Estimation\n",
        "\n",
        "The coil sensitivity maps (`smaps`) are smooth maps that show us in which parts the individual coil elements are sensitive. We will need these information for our multi-coil MRI forward and adjoint operators. We use the [python implementation](https://github.com/mikgroup/espirit-python) for ESPIRiT [1,2] to estimate these coil sensitivity maps.\n",
        "\n",
        "[1] Uecker et al. [ESPIRiTâ€”an eigenvalue approach to autocalibrating parallel MRI: Where SENSE meets GRAPPA](https://onlinelibrary.wiley.com/doi/10.1002/mrm.24751). Magn Reson Med 71(3):990-1001, 2014.\n",
        "\n",
        "[2] https://github.com/mikgroup/espirit-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP1mGhS559Zu"
      },
      "outputs": [],
      "source": [
        "import espirit\n",
        "kspace_espirit = np.transpose(kspace, (1, 2, 0))[:,:,np.newaxis]\n",
        "smaps_espirit = espirit.espirit(kspace_espirit, 8, 24, 0.05, 0)\n",
        "\n",
        "smaps = smaps_espirit[:,:,0,:,0]\n",
        "smaps = np.transpose(smaps, (2, 0, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQR3u1zulY9k"
      },
      "source": [
        "Let us visualize the coil sensitivity maps for our k-space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OS7HflObXC--",
        "outputId": "970cedb2-9465-4fb9-d7d6-98449b2e44a7"
      },
      "outputs": [],
      "source": [
        "medutils.visualization.imshow(medutils.visualization.plot_array(smaps), title='Sensitivity Maps  (compressed)', figsize=(40,20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSEJEA8GZEqK"
      },
      "source": [
        "# Multi-Coil Operators and Sensitivity-Weighted coil combination\n",
        "\n",
        "Now, we have all ingredients to combine the image which we merge into the forward and adjoint MRI multi-coil operators. The forward operator $A$ operator is defined as:\n",
        "\n",
        "$$A= \\Phi \\mathcal{F} S $$\n",
        "\n",
        "where $\\Phi$ refers to the sampling mask/trajecory, $\\mathcal{F}$ describes the Fourier transformation, and $S$ are the coil sensitivity maps. The adjoint operator $A^*$ is subsequently given as:\n",
        "\n",
        "$$A^*= S^H \\mathcal{F}^{-1} \\Phi $$\n",
        "\n",
        "**Task 3: Implement the multi-coil forward operator $A$ in `mriForwardOp(image, smaps, mask)` and adjoint operator $A^*$ in `mriAdjointOp(kspace, smaps, mask)`.**\n",
        "\n",
        "*Hint: Start with the implementation of the adjoint operator and make use of the previously written function `ifft2c`. Then, define a function `fft2c` which is the 2D centered Fourier transform before you continue with the forward operator. We perform a centered FFT with `ortho` normalization in order to provide adjoint operators.* \n",
        "\n",
        "## Suggested Readings:\n",
        "\n",
        "Pruessmann et al. [SENSE: Sensitivity encoding for fast MRI](https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291522-2594%28199911%2942%3A5%3C952%3A%3AAID-MRM16%3E3.0.CO%3B2-S) Magnetic Resonance in Medicine, 43(5):952-962, 1999.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxwj3aQBYpVi"
      },
      "outputs": [],
      "source": [
        "def mriAdjointOp(kspace, smaps, mask):\n",
        "  # TODO implement\n",
        "  return _\n",
        "\n",
        "def fft2c(image):\n",
        "  # TODO implement\n",
        "  return _\n",
        "\n",
        "def mriForwardOp(image, smaps, mask):\n",
        "  # TODO implement\n",
        "  return _"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo1OdsSInbKG"
      },
      "source": [
        "Now, you should check if the adjoint operator is working as expected. The result should be a coil-combined image. Right now, there is no undersampling mask involved, i.e., it is set to all ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "owqzxb74nVqO",
        "outputId": "3d72ddf0-1d94-4273-bc23-cca3d5656b8d"
      },
      "outputs": [],
      "source": [
        "img_cc = mriAdjointOp(kspace, smaps, np.ones_like(kspace))\n",
        "medutils.visualization.imshow(medutils.visualization.center_crop(img_cc, (nFE//2, nPE)), title='Combined image (sensitivity-weighted)', figsize=(8,8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf3LX-qSn5SP"
      },
      "source": [
        "**Task 4: Adjointness check**\n",
        "\n",
        "Now, also check if the operators are adjoint using the following equation:\n",
        "$$ \\langle Au, v\\rangle = \\langle u, A^Hv\\rangle,$$\n",
        "where $u$ and $v$ are complex random numbers. The variable $u$ should have the same size as the image $x$ and $v$ should have the same size as the k-space $y$. Note, that the sampling mask and sensitivity maps are kept constant. To create random numbers, use `np.random.randn`. Print the results for the left-hand side and right-hand side of the equation.<br/>\n",
        "\n",
        "This check is particularly important when we later want to integrate the operators into our machine learning reconstruction. In there, the operators are called in the forward as well as in the backpropagation path (or at other locations depending on the ML reconstruction type), and can hence have a significant influence on the training to skew the results or lead to numerical instabilities.  \n",
        "\n",
        "*Hint: To get the correct result, you might use the `conj` when computing the complex-valued dot product. Also, the forward and inverse Fourier transform have to be scaled the same way (`norm='ortho'`) to get correct results.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17WwxyIrn4gm",
        "outputId": "96c20959-30ae-4e5e-8dcb-8393fecad3b6"
      },
      "outputs": [],
      "source": [
        "# TODO implement the adjointness check\n",
        "\n",
        "print(f'{lhs} == {rhs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Fv8DY7ZxaZ"
      },
      "source": [
        "# Undersampling\n",
        "Now we will get to the most exciting part of this exercise - undersampling the k-space! We will generate undersampling masks for acceleration $R\\in\\lbrace 2,4\\rbrace$. \n",
        "\n",
        "**Task 5: Your task is to play around with different undersampling patterns. Generate a sampling pattern in the function `generate_mask(R, nPE, nFE, mode)` where `R` is the acceleration factor and `mode` is an integer corresponding to following patterns:**\n",
        "1. Choose randomly an integer {0,1} with propability `p=[1-1/R, 1/R]`\n",
        "2. Only set a dense block of `nRef=20` lines in the center of k-space.\n",
        "3. Combine 1.+2.\n",
        "4. Only set every `R`-th line\n",
        "5. Combine 2.+4. \n",
        "\n",
        "To create the mask, simply generate a 1D line of size `nPE`. For each item, compute and print the effective acceleration `Reff`, which is determined by `nPE` divided by the number of sampled points. \n",
        "\n",
        "The code to get the full mask of size `[nFE, nPE]` is given below.\n",
        "\n",
        "To continue the tasks on iterative reconstruction, please use `mode=3` and `R=4` as well as `mode=5` and `R=4`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08GrS1rcZw6W"
      },
      "outputs": [],
      "source": [
        "# TODO generate undersampling masks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIyzXD39zKQo"
      },
      "source": [
        "Now, generate the mask and visualize it (we visualize only a fraction in frequency encoding direction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "9nTeENqRzGig",
        "outputId": "9dd01621-72ca-41ee-a97a-3461e0006b24"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1001)\n",
        "mask = generate_mask(R=4, nPE=nPE, nFE=nFE, mode=3)\n",
        "\n",
        "medutils.visualization.imshow(mask[:40,:], 'Undersampling mask', figsize=(20,20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zy-F7VAcHYR"
      },
      "source": [
        "**Task 6: Zero-Filling solution**\n",
        "\n",
        "Now you are ready to estimate the zero filling solution by applying the adjoint operator to the data, by using the estimated undersampling mask `mask`. Play around with above mask configurations. How do the images change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "XFdaATXDb0E-",
        "outputId": "a459992a-fa45-4652-fd9f-609d4854378c"
      },
      "outputs": [],
      "source": [
        "#TODO Apply the adjoint operator to the data and use the newly created undersampling mask.\n",
        "\n",
        "\n",
        "medutils.visualization.imshow(img_cc_us, 'Undersampled image (zero filling)', figsize=(10,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFKJhOEBchbF"
      },
      "source": [
        "# Linear and Regularized Reconstruction\n",
        "Now, we are ready to implement linear and regularized reconstruction. We additionally need the gradient operator, implementing forward / backward differences in `D` and `DT`, and the multi-coil MRI forward and adjoint operators, `A` and `AH`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM58ogWNc4M6"
      },
      "outputs": [],
      "source": [
        "def nabla(x):\n",
        "    dx = np.pad(x[:,1:], [[0, 0],[0, 1]], mode='edge')\n",
        "    dy = np.pad(x[1:], [[0, 1],[0, 0]], mode='edge')\n",
        "    return np.concatenate([dx[None,...] - x, dy[None,...] - x], 0)\n",
        "\n",
        "def nablaT(x):\n",
        "    assert x.shape[0] == 2\n",
        "    dx = np.pad(x[0,:,:-1], [[0, 0],[1, 0]], mode='constant')\n",
        "    dy = np.pad(x[1,:-1], [[1, 0],[0, 0]], mode='constant')\n",
        "    return dx - x[0] + dy - x[1] \n",
        "\n",
        "D = lambda x: nabla(np.real(x)) + 1j * nabla(np.imag(x))\n",
        "DT= lambda x: nablaT(np.real(x)) + 1j * nablaT(np.imag(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anJOsdUec7cz"
      },
      "outputs": [],
      "source": [
        "A = lambda x: mriForwardOp(x, smaps, mask)\n",
        "AH = lambda x: mriAdjointOp(x, smaps, mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiFZ3aR7dT_O"
      },
      "source": [
        "## Solving the linear reconstruction problem\n",
        "Consider the following minimization problem:\n",
        "\n",
        "$$ \\min_x  E(x,y) = \\min_x \\frac{1}{2} \\Vert Ax - y \\Vert_2^2 .$$\n",
        "\n",
        "While in image denoising we are still able to compute a closed-form solution for this problem, this is not feasible for the task of MRI reconstruction anymore. We instead use first-order optimization methods and solve this by Gradient Descent:\n",
        "$$ x^{k+1} = x^{t} - \\alpha \\nabla_x E(x,y) $$\n",
        "$$ x^{k+1} = x^{k} - \\alpha A^H (Ax^k - y) $$\n",
        "\n",
        "**Task 7: Implement Gradient Descent in `opt_linear` to solve the linear reconstruction problem and run the optimization for `max_iter=50` iterations and a step size of `alpha=1`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2h7OIo1dN64"
      },
      "outputs": [],
      "source": [
        "def opt_linear(y, max_iter, alpha):\n",
        "    x = np.zeros_like(AH(y))\n",
        "\n",
        "    #TODO implement gradient descent to solve the linear reconstruction problem\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "zTkILw7GeOEW",
        "outputId": "da4d02a4-f3a8-4283-ded0-6c6243ce1993"
      },
      "outputs": [],
      "source": [
        "alpha=1\n",
        "img_linear = opt_linear(kspace, max_iter=50, alpha=alpha)\n",
        "img_linear = medutils.visualization.center_crop(img_linear, (nFE//2, nPE))\n",
        "medutils.visualization.imshow(img_linear, f'Linear reconstruction alpha={alpha}', figsize=(10,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziFJr1Mpe0re"
      },
      "source": [
        "## L2-H1 Regularization\n",
        "\n",
        "Now, we regularize the least-squares problem with a regularizer of form $\\mathcal{R}(x)=\\frac{1}{2} \\Vert \\nabla x \\Vert_2^2$.\n",
        "Consider now the following minimization problem\n",
        "\n",
        "$$ \\min_x  D(x,y) + \\lambda R(x) = \\min_x \\frac{1}{2}\\Vert Ax - y \\Vert_2^2 + \\frac{\\lambda}{2}\\Vert \\nabla x \\Vert_2^2.$$\n",
        "\n",
        "We solve this by Gradient Descent:\n",
        "$$ x^{k+1} = x^{k} - \\alpha \\left( \\nabla_x D(x,y) + \\nabla_x R(x) \\right) $$\n",
        "$$ x^{k+1} = x^{k} - \\alpha \\left( A^H (Ax^k - y) + \\lambda \\nabla^T \\nabla x^k \\right) $$\n",
        "\n",
        "**Task 8: Implement gradient descent to solve the L2-H1 regularized problem and run the optimization for `max_iter=200` iterations, a step size of `alpha=1.0` and a regularization parameter of `lambd=0.01`.**\n",
        "\n",
        "*Note that we do not have the best setting for the parameters here and the difference to the linear reconstruction might be only minimal. You can play around with the hyper-parameters. This example is to show you the properties of L2-H1 regularization and that it is actually hard to find a good set of hyper-parameters (step size, regularization parameters, iterations).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIk8_dJJe0N8"
      },
      "outputs": [],
      "source": [
        "def opt_reg_l2(y, max_iter, alpha, lambd):\n",
        "    x = np.zeros_like(AH(y))\n",
        "    # TODO implement gradient descent for L2-H1 regularization\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "6rDK8F2Ie1gO",
        "outputId": "f156d406-e81e-475d-c86b-8816ce02b2e8"
      },
      "outputs": [],
      "source": [
        "alpha = 1.0\n",
        "lambd = 0.01\n",
        "img_reg_l2 = opt_reg_l2(kspace, max_iter=200, alpha=alpha, lambd=lambd)\n",
        "img_reg_l2 = medutils.visualization.center_crop(img_reg_l2, (nFE//2, nPE))\n",
        "medutils.visualization.imshow(img_reg_l2, f'L2H1 reconstruction alpha={alpha} lambda={lambd}', figsize=(10,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgHKKicfgExG"
      },
      "source": [
        "## Sparse MRI: Wavelet Thresholding\n",
        "Medical images per se are not sparse, however, they might have a sparse representation in some transform domain. One example here is the Wavelet transform, resulting in a multi-level feature representation. We provide the `plot_wavedec` function to find out how the sparse images look like at different scales and orientations.\n",
        "\n",
        "We perform an optimization first wrt. data consistency term. This is followed by a Wavelet transform, and the *detailed* Wavelet coefficients are surpressed by using soft-thresholding, i.e.,\n",
        "\n",
        "$$\n",
        "\\text{thresh}(x) = \\frac{x}{\\vert x \\vert}\\max(\\vert x \\vert - \\alpha\\lambda , 0)\n",
        "$$\n",
        "\n",
        "**Task 9: Implement the soft-thresholding in `soft_thresh(x, tau)`**\n",
        "\n",
        "*Hint: Note, that the absolut value could get zero, and a small epsilon might be adorable to surpress this.*\n",
        "\n",
        "### Suggested Readings\n",
        "\n",
        "Lustig et al. [Compressed Sensing MRI](https://ieeexplore.ieee.org/document/4472246), IEEE Signal Processing Magazine 25(2):72-82, 2008.\n",
        "\n",
        "Lustig et al. [Sparse MRI: The application of compressed sensing for rapid MR imaging](https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.21391). Magnetic Resonance in Medicine 58(6):1182-1195, 2007."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17LiCd6A_MOf"
      },
      "outputs": [],
      "source": [
        "def soft_thresh(x, tau):\n",
        "    #TODO: implement soft-thresholding\n",
        "    return _"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIeuyuFtgEMm"
      },
      "outputs": [],
      "source": [
        "import pywt\n",
        "\n",
        "def plot_wavedec(img, wavelet='db4', level=2):\n",
        "    img = medutils.visualization.center_crop(img_cc, (nFE//2, nPE))\n",
        "    coeffs = pywt.wavedecn(img, wavelet=wavelet, level=level)\n",
        "    # normalize coeffs\n",
        "    coeffs[0] /= np.max(np.abs(coeffs[0]))\n",
        "    for level in range(1, len(coeffs)):\n",
        "        for key in coeffs[level].keys():\n",
        "            coeffs[level][key] /= np.max(np.abs(coeffs[level][key]))\n",
        "    arr, coeff_slices = pywt.coeffs_to_array(coeffs)\n",
        "    medutils.visualization.imshow(arr, figsize=(10,10))\n",
        "\n",
        "def opt_reg_wavelet(y, max_iter, alpha, lambd, wavelet='db4', level=3):\n",
        "    x = np.zeros_like(AH(y))\n",
        "    wavelet_object = pywt.Wavelet(wavelet)\n",
        "    threshold = alpha * lambd\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        x = x - alpha * (AH(A(x) - y))\n",
        "        coeffs = pywt.wavedecn(x, wavelet_object, level=level)\n",
        "        array, coeff_slices = pywt.coeffs_to_array(coeffs)\n",
        "        denoised_array=soft_thresh(array, threshold)\n",
        "        denoised_coeffs = pywt.array_to_coeffs(denoised_array, coeff_slices, output_format='wavedecn')\n",
        "        denoised_coeffs[0] = coeffs[0]\n",
        "        x = pywt.waverecn(denoised_coeffs, wavelet_object)\n",
        "        \n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlHCMCmB_ECn"
      },
      "source": [
        "Next, we define a wavelet, the number of levels for decomposition and plot the decomposition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "W2RfjPmygfCw",
        "outputId": "9fa85383-6694-45e4-d69f-2745f701109d"
      },
      "outputs": [],
      "source": [
        "# Plot Wavelet Decomposition\n",
        "wavelet='bior2.8'\n",
        "level=3\n",
        "plot_wavedec(img_cc, wavelet, level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaB2pREX_Y0j"
      },
      "source": [
        "Finally, we run the optimization for `lambd=1e-6` and `alpha=1` and 200 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "wnDICemEgub_",
        "outputId": "b0cc11f2-bab3-4531-9872-74075e82bbe3"
      },
      "outputs": [],
      "source": [
        "lambd=1e-6\n",
        "alpha=1.0\n",
        "img_reg_wavelet = opt_reg_wavelet(kspace, max_iter=200, alpha=alpha, lambd=lambd, wavelet=wavelet, level=level)\n",
        "img_reg_wavelet = medutils.visualization.center_crop(img_reg_wavelet, (nFE//2, nPE))\n",
        "medutils.visualization.imshow(img_reg_wavelet, f'Wavelet reconstruction alpha={alpha} lambda={lambd}', figsize=(10,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine learning reconstruction\n",
        "After we are able to run a conventional MR image reconstruction, we want to work on a learnable regularizer. For sake of simplicity and computational runtime, we will use the MNIST database (handwritten digits), for which we will create \"fake\" k-spaces. The same principles and code work however for any real MRI data, for example from public sources: [fastMRI](https://fastmri.med.nyu.edu/), [MRIData](http://mridata.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learnable denoiser (real-valued)\n",
        "We start of by learning a magnitude-based (real-valued) denoiser. We first define the data pipelines to feed the data into training, validation and test set. The MNIST database is used for showcasing. A white Gaussian noise is simulated retrospectively and added to the data. The task of the network is to denoise the images with real-valued operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Database pipeline\n",
        "First define the data pipelines (in the form of dataset classes) for training, validation and test set. Retrospective noise simulation is performed inside the dataset class. You may use the provided dataset links to MNIST from [Torch](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html) or [Tensorflow](https://www.tensorflow.org/datasets/catalog/mnist). We have prepared for you already the data pipeline `DataGeneratorMNIST()`. We will showcase the Torch and merlinth implementation. A similar example can be performed with Tensorflow and merlintf.\n",
        "\n",
        "**Task 10: Implement the retrospective noise simulation in `_noisy()`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "\n",
        "class DataGeneratorMNIST(Dataset):\n",
        "    'Generates real-valued data for keras model'\n",
        "\n",
        "    def __init__(self, noise_level=0.1, mode='train'):\n",
        "        'Initialization'\n",
        "        assert mode in ['train', 'val', 'test']\n",
        "\n",
        "        self.noise_level = noise_level\n",
        "        if not hasattr(self, 'dtype'):\n",
        "            self.dtype = np.float32\n",
        "\n",
        "        if mode in ['train', 'val']:\n",
        "            self.img = datasets.MNIST(root='./data', train=True, download=True).data\n",
        "        else:\n",
        "            self.img = datasets.MNIST(root='./data', train=False, download=True).data\n",
        "\n",
        "        # ATTENTION!\n",
        "        # This code part is performing a random database splitting for training/validation, as all MNIST images\n",
        "        # are independent of each other. In case of MRI, a patient leave-out approach needs to be performed!!!\n",
        "        # predefined 80/20 split. MNIST has 60000 samples\n",
        "        cv_split = {'train': 48000, 'val': 12000}\n",
        "        if mode == 'train':\n",
        "            self.img = self.img[0:cv_split['train']]\n",
        "        elif mode == 'val':\n",
        "            self.img = self.img[-cv_split['val']:]\n",
        "\n",
        "        # prepare data\n",
        "        self._prepare_data()\n",
        "\n",
        "        # save the size of the images\n",
        "        self.dim = self.img.shape[1:]\n",
        "\n",
        "        # get amount of data samples in train/val/test\n",
        "        self.n_samples = self.img.shape[0]\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        'Data preparation'\n",
        "        # convert uint8 to float32\n",
        "        self.img = self.img.to(torch.float32) / 255.0\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of samples'\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        sample = self.img[index]\n",
        "\n",
        "        # Get target\n",
        "        target = sample.unsqueeze(0)  # Add channel dimension\n",
        "\n",
        "        # Generate noisy input\n",
        "        noisy = self._noisy(sample).unsqueeze(0)   # Add channel dimension\n",
        "\n",
        "        return noisy, target\n",
        "\n",
        "    def _noisy(self, x):\n",
        "        'Real-valued noise simulation'\n",
        "        #TODO: implement the noise simulation\n",
        "        # add Gaussian noise\n",
        "        return _"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define the actual data pipelines for training, validation and test. We will use the Torch [`DataLoader`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders) that builds the batches for our network input and calls our `DataGeneratorMNIST` dataset.\n",
        "\n",
        "**Task 11: Write the `training_generator` (batch size 32, noise level 0.5), `validation_generator` (batch size 32, noise level 0.5), and `test_generator` (batch size 1, noise level 0.5)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# initialize some parameters\n",
        "noise_level = 0.5  # simulated additive white Gaussian noise level\n",
        "\n",
        "# Data Generators (Data pipeline) for real-valued data\n",
        "# training set\n",
        "#TODO: implement the data generator for training\n",
        "training_generator = \n",
        "\n",
        "# validation set\n",
        "#TODO: implement the data generator for validation\n",
        "validation_generator = \n",
        "\n",
        "# test set\n",
        "# ideally testing should be performed on real noisy cases and not simulated ones\n",
        "#TODO: implement the data generator for testing\n",
        "test_generator = \n",
        "\n",
        "print('Training batches to process:', len(training_generator))\n",
        "print('Validation batches to process:', len(validation_generator))\n",
        "print('Test samples to process:', len(test_generator))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n",
        "Now let us define a simple denoiser architecture in the form of a convolutional neural network (CNN). \n",
        "\n",
        "**Task 12: Define a 3-layer CNN model with ReLU activation function. Define its corresponding inputs and outputs. Each layer shall have 8 filters, a kernel size of 3x3, no striding, and a ReLU activation function.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the input tensor (batch_size=1, channels=1, height=28, width=28)\n",
        "input_tensor = torch.randn(1, 1, 28, 28)  # Example input\n",
        "\n",
        "# Layer 1: Convolution + ReLU\n",
        "\n",
        "\n",
        "# Layer 2: Convolution + ReLU\n",
        "\n",
        "\n",
        "# Layer 3: Convolution + ReLU\n",
        "\n",
        "\n",
        "# Print the output shape\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "or directly as `nn.Module` class, with layer initialization in `__init__` and a feed-forward path defined in `forward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Model\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the 3-layer CNN model in PyTorch\n",
        "class ThreeLayerCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ThreeLayerCNN, self).__init__()\n",
        "        # TODO: Define layers\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Define the forward path\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = ThreeLayerCNN()\n",
        "\n",
        "# Print model summary\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also extend this 3-layer CNN to a 3-layer ResNet (residual network), in which a residual connection spans from input to the output. This is one way to avoid gradient vanishing for deeper networks.\n",
        "\n",
        "**Task 13: Define the 3-layer ResNet (same layer parameters as before).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the 3-layer Residual CNN model in PyTorch\n",
        "class Residual3LayerCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Residual3LayerCNN, self).__init__()\n",
        "        # TODO: Define layers\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Define the forward path\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = Residual3LayerCNN()\n",
        "\n",
        "# Print model summary\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to train the model we need to prepare the optimizer, loss function and validation metrics.\n",
        "\n",
        "**Task 14: Define an Adam optimizer (learning rate 0.001), MSE loss function, and MSE and MAE for validation metrics.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss function\n",
        "criterion = _  # Mean Squared Error loss\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer =   # Adam optimizer with learning rate 0.001\n",
        "\n",
        "# Metrics (for evaluation during training/validation)\n",
        "def mse_metric(output, target):\n",
        "    return _\n",
        "\n",
        "def mae_metric(output, target):\n",
        "    return _"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training\n",
        "\n",
        "In order to train the model, we first need to build the training and validation [loop](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop) as a function `train_model`. \n",
        "\n",
        "**Task 15: Build the training and validation loop for all epochs. Train the configured 3-layer ResNet model over 3 epochs. Monitor training progress with the validation set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Define the training loop\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, mse_metric=mse_metric, mae_metric=mae_metric):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        train_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        with tqdm(total=len(train_loader), desc=\"Training\", unit=\"batch\") as pbar:\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)  # Move data to device (e.g., GPU)\n",
        "                # TODO implement the training path\n",
        "\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                pbar.update(1)  # Update progress bar\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        val_mse = 0.0\n",
        "        val_mae = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                # TODO implement the validation path\n",
        "                \n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_mse += mse_metric(outputs, targets)\n",
        "                val_mae += mae_metric(outputs, targets)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
        "              f\"Validation Loss: {val_loss / len(val_loader):.4f}, \"\n",
        "              f\"Validation MSE: {val_mse / len(val_loader):.4f}, \"\n",
        "              f\"Validation MAE: {val_mae / len(val_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)  # Move model to device\n",
        "train_model(model, training_generator, validation_generator, criterion, optimizer, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test\n",
        "To display the performance metrics on the test set, we need to build a test loop (similar to the training loop), but without gradient backpropagation (`torch.no_grad()`). Afterwards we can test the trained model to predict a denoised output. \n",
        "\n",
        "**Task 16: Evaluate the model over the complete test set and run a model prediction for a single case.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained model\n",
        "def evaluate_model(model, test_loader, criterion, mse_metric=mse_metric, mae_metric=mae_metric):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    test_mse = 0.0\n",
        "    test_mae = 0.0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to device (e.g., GPU)\n",
        "            outputs =   # TODO: Forward path\n",
        "            loss =   # TODO: Compute loss\n",
        "            test_loss += loss.item()\n",
        "            test_mse += mse_metric(outputs, targets)\n",
        "            test_mae += mae_metric(outputs, targets)\n",
        "\n",
        "    # Compute average metrics\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "    avg_mse = test_mse / len(test_loader)\n",
        "    avg_mae = test_mae / len(test_loader)\n",
        "\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test MSE: {avg_mse:.4f}, Test MAE: {avg_mae:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "evaluate_model(model, test_generator, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict with trained model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    inputs, targets =   # TODO: Get a batch from the test generator\n",
        "    inputs, targets = inputs.to(device), targets.to(device)  # Move data to the appropriate device\n",
        "    predicted_output =   # TODO: Forward path\n",
        "\n",
        "# Display the predicted output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "icase = 0  # Display the first example\n",
        "plt.figure()\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(inputs[icase, 0].cpu().numpy(), cmap='gray')  # Noisy input\n",
        "plt.title('Noisy')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.imshow(predicted_output[icase, 0].cpu().numpy(), cmap='gray')  # Reconstructed output\n",
        "plt.title('Recon')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.imshow(targets[icase, 0].cpu().numpy(), cmap='gray')  # Ground truth\n",
        "plt.title('Target')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learnable denoiser (complex-valued data with 2-channel real-valued processing)\n",
        "In many applications, denoising on the magnitude (real-valued) image can be sufficient. However, in order to proceed to an integration with data consistency, we need to consider the complex-valued nature of the MR data. The complex-valued data can be handled in the network either as real-valued operations (2-channel real-valued) or as complex-valued operations. We will examine the difference between them.\n",
        "Let us start with the 2-channel real-valued network. We first define the data pipelines to feed the data into training, validation and test set. The MNIST database is used for showcasing. Since MNIST are real-valued images, a phase is simulated and added to the images to generate a complex-valued input. A white Gaussian noise is simulated retrospectively and added to the data. The task of the network is to denoise the images with real-valued operations, i.e. complex data is stored in channel dimension as 2 real-valued tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Database pipeline\n",
        "First define the data pipelines (in the form of generator functions) for training, validation and test set. Retrospective noise simulation is performed inside the generator functions. We have prepared for you already the data pipeline `TwoChannelDataGeneratorMNIST(DataGeneratorMNIST)`.\n",
        "\n",
        "**Task 17: Implement the retrospective complex-valued noise simulation in `_noisy()`**\n",
        "\n",
        "**Task 18: Implement the complex-valued to 2-channel real-valued conversion into the data pipeline.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwoChannelDataGeneratorMNIST(DataGeneratorMNIST):\n",
        "    \"\"\"Generates complex-valued data for PyTorch model\"\"\"\n",
        "\n",
        "    def __init__(self, noise_level=0.1, mode='train'):\n",
        "        \"\"\"Initialization\"\"\"\n",
        "        super().__init__(noise_level=noise_level, mode=mode)\n",
        "        self.dtype = torch.float32\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        'Data preparation'\n",
        "        # Convert uint8 to float32 and normalize\n",
        "        super()._prepare_data()\n",
        "        # Simulate some phase information\n",
        "        self.img = self.img + 1j * self.img.flip(0)\n",
        "        self.img = self._normalize(self.img)\n",
        "\n",
        "    def _complex2real(self, z, channel_last=False):\n",
        "        'Convert complex-valued data to real-valued data'\n",
        "        stack_dim = -1 if channel_last else 0\n",
        "        # TODO: implement the conversion\n",
        "\n",
        "        return _\n",
        "\n",
        "    def _normalize(self, x, min=0, max=1):\n",
        "        'Normalization'\n",
        "        if torch.is_complex(x):\n",
        "            eps = 1e-9\n",
        "            xabs = torch.abs(x)\n",
        "            normed_magn = (xabs - xabs.min()) / (xabs.max() - xabs.min()) * (max - min) + min\n",
        "            normed_magn = torch.clamp(normed_magn, min=eps)\n",
        "            phase = torch.angle(x)\n",
        "            return normed_magn * torch.exp(1j * phase)\n",
        "        else:\n",
        "            return (x - x.min()) / (x.max() - x.min()) * (max - min) + min\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        sample = self.img[index]\n",
        "\n",
        "        # Get target\n",
        "        target = self._complex2real(sample)  # complex to real\n",
        "\n",
        "        # Generate noisy input\n",
        "        noisy = self._complex2real(self._noisy(sample))  # complex to real\n",
        "\n",
        "        return noisy, target\n",
        "\n",
        "    def _noisy(self, x):\n",
        "        'Complex-valued noise simulation'\n",
        "        # Add complex Gaussian noise\n",
        "        # TODO: implement the noise simulation\n",
        "\n",
        "        return x + noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define the actual data pipelines for training, validation and test. \n",
        "\n",
        "**Task 19: Write the `training_generator`, `validation_generator`, and `test_generator` similar to the real-valued case.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# initialize some parameters\n",
        "noise_level = 0.5  # simulated additive white Gaussian noise level\n",
        "\n",
        "# Data Generators (Data pipeline) for real-valued data\n",
        "# training set\n",
        "#TODO: implement the data generator for training\n",
        "training_generator = \n",
        "\n",
        "# validation set\n",
        "#TODO: implement the data generator for validation\n",
        "validation_generator = \n",
        "\n",
        "# test set\n",
        "# ideally testing should be performed on real noisy cases and not simulated ones\n",
        "#TODO: implement the data generator for testing\n",
        "test_generator = \n",
        "\n",
        "print('Training batches to process:', len(training_generator))\n",
        "print('Validation batches to process:', len(validation_generator))\n",
        "print('Test samples to process:', len(test_generator))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n",
        "Now let us define a simple denoiser architecture in the form of a convolutional neural network (CNN). \n",
        "\n",
        "**Task 20: Define a 3-layer CNN model with its corresponding inputs and outputs - similar to the real-valued case.** (Hint: All operations are still real-valued.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Model\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the 3-layer CNN model in PyTorch\n",
        "class ThreeLayerCNN2ch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ThreeLayerCNN2ch, self).__init__()\n",
        "        # TODO: Define layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Define the forward path\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = ThreeLayerCNN2ch()\n",
        "\n",
        "# Print model summary\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us extend the architecture to a 3-layer ResNet (residual network).\n",
        "\n",
        "**Task 21: Define the 3-layer ResNet (similar to real-valued case).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the 3-layer Residual CNN model in PyTorch\n",
        "class Residual3LayerCNN2ch(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Residual3LayerCNN2ch, self).__init__()\n",
        "        # TODO: Define layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Define the forward path\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = Residual3LayerCNN2ch()\n",
        "\n",
        "# Print model summary\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training\n",
        "For training the 2-channel real-valued model, we can reuse the previous training loop, loss function and the optimizer definition.\n",
        "\n",
        "**Task 22: Train the 3-layer ResNet model with an Adam optimizer (learning rate 0.001) for 3 epochs using an MSE loss function, and apply MSE and MAE as validation metrics.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = # TODO: Define the loss function\n",
        "optimizer = # TODO: Define the optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)  # Move model to device\n",
        "train_model(model, training_generator, validation_generator, criterion, optimizer, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test\n",
        "\n",
        "**Task 23: Test the trained model to predict a denoised output and to display performance (metrics) on the test set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate trained model\n",
        "evaluate_model(model, test_generator, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict with trained model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    inputs, outputs =  # TODO: Get a batch from the test generator\n",
        "    inputs, outputs = inputs.to(device), outputs.to(device)  # Move data to the appropriate device\n",
        "    predicted_output = # TODO: Forward path\n",
        "\n",
        "# Display the predicted output\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "icase = 0  # Display the first example\n",
        "plt.figure()\n",
        "\n",
        "# Magnitude - Noisy\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(np.abs(inputs[icase, 0].cpu().numpy()), cmap='gray')\n",
        "plt.title('Magnitude - Noisy')\n",
        "plt.axis('off')\n",
        "\n",
        "# Magnitude - Recon\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.imshow(np.abs(predicted_output[icase, 0].cpu().numpy()), cmap='gray')\n",
        "plt.title('Magnitude - Recon')\n",
        "plt.axis('off')\n",
        "\n",
        "# Magnitude - Target\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.imshow(np.abs(outputs[icase, 0].cpu().numpy()), cmap='gray')\n",
        "plt.title('Magnitude - Target')\n",
        "plt.axis('off')\n",
        "\n",
        "# Phase - Noisy\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.imshow(np.angle(inputs[icase, 0].cpu().numpy()), vmin=-np.pi, vmax=np.pi)\n",
        "plt.title('Phase - Noisy')\n",
        "plt.axis('off')\n",
        "\n",
        "# Phase - Recon\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.imshow(np.angle(predicted_output[icase, 0].cpu().numpy()), vmin=-np.pi, vmax=np.pi)\n",
        "plt.title('Phase - Recon')\n",
        "plt.axis('off')\n",
        "\n",
        "# Phase - Target\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.imshow(np.angle(outputs[icase, 0].cpu().numpy()), vmin=-np.pi, vmax=np.pi)\n",
        "plt.title('Phase - Target')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learnable denoiser (complex-valued data with complex-valued processing)\n",
        "Let us proceed now to a full complex-valued processing. We first define the data pipelines to feed the data into training, validation and test set. The MNIST database is used for showcasing. Since MNIST are real-valued images, a phase is simulated and added to the images to generate a complex-valued input. A white Gaussian noise is simulated retrospectively and added to the data. The task of the network is to denoise the images with complex-valued operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Database pipeline\n",
        "First define the data pipelines (in the form of generator functions) for training, validation and test set. Retrospective noise simulation is performed inside the generator functions. You can reuse your previous `TwoChannelDataGeneratorMNIST(DataGeneratorMNIST)` and only need to change a single line.\n",
        "\n",
        "**Task 24: Implement the `ComplexDataGeneratorMNIST(TwoChannelDataGeneratorMNIST)`, and provide the respective data loader pipelines for training, validation and test.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ComplexDataGeneratorMNIST(TwoChannelDataGeneratorMNIST):\n",
        "    \"\"\"Generates complex-valued data for PyTorch model\"\"\"\n",
        "\n",
        "    def __init__(self, noise_level=0.1, mode='train'):\n",
        "        \"\"\"Initialization\"\"\"\n",
        "        super().__init__(noise_level=noise_level, mode=mode)\n",
        "        self.dtype = torch.complex64\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\"\"\"\n",
        "        sample = self.img[index]\n",
        "\n",
        "        # Get target\n",
        "        # TODO: Get target image, Hint: do not forget to add the channel dimension\n",
        "        target =  # Add channel dimension\n",
        "\n",
        "        # Generate noisy input\n",
        "        # TODO: Get noisy image, Hint: do not forget to add the channel dimension\n",
        "        noisy =  # Add channel dimension\n",
        "\n",
        "        return noisy, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# initialize some parameters\n",
        "noise_level = 0.5  # simulated additive white Gaussian noise level\n",
        "\n",
        "# Data Generators (Data pipeline) for complex-valued data\n",
        "# training set\n",
        "#TODO: implement the data generator for training\n",
        "training_generator = \n",
        "\n",
        "# validation set\n",
        "#TODO: implement the data generator for validation\n",
        "validation_generator = \n",
        "\n",
        "# test set\n",
        "# ideally testing should be performed on real noisy cases and not simulated ones\n",
        "#TODO: implement the data generator for testing\n",
        "test_generator = \n",
        "\n",
        "print('Training batches to process:', len(training_generator))\n",
        "print('Validation batches to process:', len(validation_generator))\n",
        "print('Test samples to process:', len(test_generator))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n",
        "Now let us define a convolutional neural network (CNN) denoiser that uses complex-valued operations and a cReLU activation function. We will use the [MERLIN](https://github.com/midas-tum) package to provide the complex-valued operations.\n",
        "\n",
        "**Task 25: Define a 3-layer complex-valued CNN model with its corresponding inputs and outputs. Each layer shall have 8 filters, a kernel size of 3x3, no striding, and a cReLU activation function.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import merlinth\n",
        "import torch.nn as nn\n",
        "from merlinth.layers.complex_act import cReLU\n",
        "\n",
        "# Define the 3-layer complex-valued CNN model in PyTorch\n",
        "class ThreeLayerCNNc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ThreeLayerCNNc, self).__init__()\n",
        "        # TODO: Define the layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Define the forward path\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = ThreeLayerCNNc()\n",
        "\n",
        "# Print model summary\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Task 26: Extend the architecture to a 3-layer complex-valued ResNet (residual network).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the 3-layer complex-valued ResNet model in PyTorch\n",
        "class Residual3LayerCNNc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Residual3LayerCNNc, self).__init__()\n",
        "        # TODO: Define the layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Define the forward path\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = Residual3LayerCNNc()\n",
        "\n",
        "# Print model summary\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training\n",
        "Before we can train the model, we need to make sure to use a complex-valued MSE and MAE. You can either define it yourself or use the ones provided by [merlinth](https://github.com/midas-tum/merlin/blob/master/pytorch/merlinth/losses/pairwise_loss.py).\n",
        "\n",
        "**Task 27: Assign an Adam optimizer (learning rate 0.001) with complex-valued MSE loss function. Use the MSE and MAE for validation metrics. Train for 3 epochs.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from merlinth.losses import mse, mae\n",
        "\n",
        "criterion = # TODO: Define the loss function\n",
        "optimizer = # TODO: Adam optimizer with learning rate 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)  # Move model to device\n",
        "train_model(model, training_generator, validation_generator, criterion, optimizer, epochs=3, mse_metric=mse, mae_metric=mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test\n",
        "\n",
        "**Task 28: Test the trained model to predict a denoised output and to display performance (metrics) on the test set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from merlinth.losses import mse, mae\n",
        "# evaluate trained model\n",
        "evaluate_model(model, test_generator, criterion, mse_metric=mse, mae_metric=mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict with trained model (Torch version)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    inputs, outputs =  # TODO: Get a batch from the test generator\n",
        "    inputs, outputs = inputs.to(device), outputs.to(device)  # Move data to the appropriate device\n",
        "    predicted_output =  # TODO: Forward path\n",
        "\n",
        "# Display the predicted output\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "icase = 0  # Display the first example\n",
        "plt.figure()\n",
        "\n",
        "# Magnitude - Noisy\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(np.abs(inputs[icase, 0].cpu().numpy()), cmap='gray')\n",
        "plt.title('Magnitude - Noisy')\n",
        "plt.axis('off')\n",
        "\n",
        "# Magnitude - Recon\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.imshow(np.abs(predicted_output[icase, 0].cpu().numpy()), cmap='gray')\n",
        "plt.title('Magnitude - Recon')\n",
        "plt.axis('off')\n",
        "\n",
        "# Magnitude - Target\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.imshow(np.abs(outputs[icase, 0].cpu().numpy()), cmap='gray')\n",
        "plt.title('Magnitude - Target')\n",
        "plt.axis('off')\n",
        "\n",
        "# Phase - Noisy\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.imshow(np.angle(inputs[icase, 0].cpu().numpy()), vmin=-np.pi, vmax=np.pi, cmap='hsv')\n",
        "plt.title('Phase - Noisy')\n",
        "plt.axis('off')\n",
        "\n",
        "# Phase - Recon\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.imshow(np.angle(predicted_output[icase, 0].cpu().numpy()), vmin=-np.pi, vmax=np.pi, cmap='hsv')\n",
        "plt.title('Phase - Recon')\n",
        "plt.axis('off')\n",
        "\n",
        "# Phase - Target\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.imshow(np.angle(outputs[icase, 0].cpu().numpy()), vmin=-np.pi, vmax=np.pi, cmap='hsv')\n",
        "plt.title('Phase - Target')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Physics-based unrolled network (complex-valued data with complex-valued operations)\n",
        "Now we have all ingredients ready to put our learnable regularizer inside an unrolled reconstruction network. We will go over the steps of defining the architecture and setting up the data consistency layer. \n",
        "It is also conceivable to use the trainable network inside any other reconstruction scheme. For another example of a Plug and Play Prior, please refer to this [tutorial](https://github.com/ISMRM-MIT-CMR/CMR-DL-challenge/blob/master/challenge_plug_and_play_sample_solution.ipynb). \n",
        "\n",
        "We first define the data pipelines to feed the data into training, validation and test set. The MNIST database is used for showcasing. Since MNIST are real-valued images, a phase is simulated and added to the images to generate a complex-valued input. The fourier transformed image serves as k-space for data consistency. Single-MR-coil processing is performed. A retrospective undersampling according to a Parallel Imaging (PI) or Compressed Sensing (CS) like trajectory is simulated and applied to the data. The task of the network is to reconstruct the undersampled data with complex-valued operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Database pipeline\n",
        "First define the data pipelines (in the form of generator functions) for training, validation and test set. Retrospective undersampling is performed inside the generator functions. You can reuse your previous `ComplexDataGeneratorMNIST(TwoChannelDataGeneratorMNIST)` and the subsampling implementations from above. Before we can start with the dataloader we need to either extend the `ifft2c` and `fft2c` functions from above to account for the network channel dimension and operate on Tensors. Alternatively, you can also use the implementations provided by [MERLIN](https://github.com/midas-tum/merlin/blob/master/pytorch/merlinth/layers/fft.py). \n",
        "\n",
        "**Task 29: Implement an `fftnc` and `ifftnc` to operate on each axis of the Tensor.** <br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement the torch-compatible FFT and IFFT functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the dataloader, we need to consider that in the case of a physics-based unrolled network, we want to provide the noisy/undersampled image, undersampled k-space, sampling mask, and sensitivy map (if multi-coil) as network input. You want to be able to provide an acceleration range ([minimum acceleration, maximum acceleration] between which a random acceleration is selected), acceleration type (`PI` or `CS`), the number of fully-sampled center lines, and the mode (`train`, `eval`, `test`). \n",
        "\n",
        "**Task 30: Implement the dataset class `ComplexRawDataGeneratorMNIST(ComplexDataGeneratorMNIST)`.** <br/>\n",
        "\n",
        "To perform the retrospective undersampling, we need to integrate a `_subsample` function that operates on Tensors. You can adjust the above subsampling function modes 3 (CS) and 5 (PI).\n",
        "\n",
        "**Task 31: Implement the CS and PI subsampling into the `_subsample` function in `ComplexRawDataGeneratorMNIST(ComplexDataGeneratorMNIST)`.** <br/>\n",
        "**Task 32: Provide a `__getitem__` function to return `[noisy, kspace, mask], target`.** <br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from scipy.stats import norm as scs_norm\n",
        "\n",
        "class ComplexRawDataGeneratorMNIST(ComplexDataGeneratorMNIST):\n",
        "    \"\"\"Generates complex-valued data with raw data (for data consistency) for PyTorch model\"\"\"\n",
        "\n",
        "    def __init__(self, accelerations=[2, 4], accel_type='PI', center=20, mode='train'):\n",
        "        \"\"\"Initialization\"\"\"\n",
        "        super().__init__(noise_level=0, mode=mode)\n",
        "        self.dtype = torch.complex64\n",
        "        self.accelerations = accelerations\n",
        "        self.accel_type = accel_type\n",
        "        self.center = int(center) if center > 1 else int(center * 28)  # center can be lines or percentage, assuming MNIST image size of 28x28\n",
        "        self.kspace = self._generate_kspace()\n",
        "\n",
        "    def _generate_kspace(self):\n",
        "        \"\"\"Generate k-space from images\"\"\"\n",
        "        return merlinth.layers.fft2c(self.img).detach().clone().to(dtype=self.dtype, device=self.img.device)\n",
        "\n",
        "    def _subsample(self, kspace):\n",
        "        \"\"\"Retrospective undersampling/sub-Nyquist sampling\"\"\"\n",
        "        #kspace = torch.tensor(kspace, dtype=self.dtype, device=self.img.device)\n",
        "        mask = torch.zeros(self.img.shape[1:], dtype=torch.float32, device=kspace.device)\n",
        "\n",
        "        # Fully sampled center\n",
        "        fscenter = (self.img.shape[2] // 2 - self.center // 2, self.img.shape[2] // 2 + self.center // 2)\n",
        "        mask[:, fscenter[0]:fscenter[1]] = 1\n",
        "        R = np.random.choice(self.accelerations)\n",
        "\n",
        "        if self.accel_type == 'PI':\n",
        "            # TODO: Parallel Imaging undersampling\n",
        "            \n",
        "        elif self.accel_type == 'CS':\n",
        "            # TODO: Compressed Sensing like undersampling\n",
        "\n",
        "        kspace_us = (kspace * mask).detach().clone().to(dtype=self.dtype, device=kspace.device)\n",
        "        return kspace_us, mask\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one data sample\"\"\"\n",
        "        sample = self.img[index]\n",
        "        kspace = self.kspace[index]  # Get k-space for the sample\n",
        "\n",
        "        # TODO: Get target, Hint: do not forget to add the channel dimension\n",
        "        target =   # Add channel dimension\n",
        "\n",
        "        # TODO: Subsample k-space\n",
        "        kspace_us, mask =  # kspace and mask do not need a channel dimension\n",
        "\n",
        "        # TODO: Generate noisy input, Hint: do not forget to add the channel dimension\n",
        "        noisy = # Add channel dimension\n",
        "\n",
        "        return [noisy, kspace_us, mask], target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Task 33: Provide the respective data loader pipelines for training, validation and test.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# initialize some parameters\n",
        "accelerations = [2, 4]  # simulate retrospectively accelerations in the range of e.g. 2x to 4x\n",
        "accel_type = 'PI'  # simulated undersampling strategy: 'PI' = Parallel Imaging, 'CS' = Compressed Sensing\n",
        "center = 20  # numbers of center lines or percent of fully sampled central region along ky phase-encoding, e.g. 0.1 := floor(10% * 28) ky center lines = 2 ky center lines \n",
        "\n",
        "# Data Generators (Data pipeline) for complex-valued data\n",
        "# training set\n",
        "#TODO: implement the data generator for training\n",
        "training_generator = \n",
        "\n",
        "# validation set\n",
        "#TODO: implement the data generator for validation\n",
        "validation_generator = \n",
        "\n",
        "# test set\n",
        "# ideally testing should be performed on real noisy cases and not simulated ones\n",
        "#TODO: implement the data generator for testing\n",
        "test_generator = )\n",
        "\n",
        "print('Training batches to process:', len(training_generator))\n",
        "print('Validation batches to process:', len(validation_generator))\n",
        "print('Test samples to process:', len(test_generator))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n",
        "For the learnable regularizer we choose our previously defined complex-valued 3-layer residual network from Task 26.\n",
        "\n",
        "Next we can work on the data consistency layer. You may either implement a conjugate gradient algorithm yourself or rely on the functions/data consistency layer provided within MERLIN: `merlinth.layers.DCPM`.\n",
        "The conjugate gradient layer solves the data consistency term\n",
        "\n",
        "$$ \\min_x \\frac{1}{2}\\Vert Ax - y \\Vert_2^2 $$\n",
        "\n",
        "with a pre-conditioned prior $x_{denoise}$ on $x$, i.e. our denoised image output of the learnable regularizer. The problem can be equally stated as the regular conjugate gradient ($Ex=b$) with the preconditioning:\n",
        "\n",
        "$$ A^H A x = A^H m + \\lambda x_{denoise} $$\n",
        "\n",
        "We suggest the following literature:\n",
        "- Pruessmann, K. P.; Weiger, M.; Boernert, P. and Boesiger, P. [Advances in sensitivity encoding with arbitrary k-space trajectories](https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.1241?sid=nlm%3Apubmed). Magn Reson Med 46: 638-651 (2001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us first have a look at the forward and adjoint operators. You can either implement these single-coil (for the provided example data) operators yourself or use the ones provided by [MERLIN](https://github.com/midas-tum/merlin/blob/master/pytorch/merlinth/layers/mri.py).\n",
        "\n",
        "**Task 34: Implement the forward and adjoint operators.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Implement the forward and adjoint operators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Task 35: Implement the data consistency layer and build the unrolled network class as a `torch.nn.Module` class, which contains the learnable regularizer (previous Task 26) and the data consistency layer both unrolled by the amount of unrolls. Alternatively, you may toggle via a variable `is_residual` between the complex-valued 3-layer residual network (Task 26) and the complex-valued 3-layer CNN (Task 25). To stack multiple modules in a list, please use [`nn.ModuleList`](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html). You may implement weight sharing of the learned regularizers or cascading individual regularizers. Please define the final unrolled model with 5 cascades.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from merlinth.layers import DCPM\n",
        "from merlinth.layers.mri import ForwardOp, AdjointOp\n",
        "\n",
        "# Define unrolled reconstruction model\n",
        "class UnrolledNetwork(nn.Module):\n",
        "    def __init__(self, cascades, shared_params=True, is_residual=True):\n",
        "        super(UnrolledNetwork, self).__init__()\n",
        "        \n",
        "        self.T = 1 if shared_params else cascades  # shared denoiser network or (new) cascaded denoisers\n",
        "        self.T_end = cascades  # number of cascades\n",
        "        \n",
        "        # Prepare denoising networks\n",
        "        if is_residual:\n",
        "            self.denoiser = # TODO: Residual CNN\n",
        "        else:\n",
        "            self.denoiser = # TODO: CNN\n",
        "\n",
        "        A = # TODO: ForwardOp\n",
        "        AH = # TODO: AdjointOp\n",
        "\n",
        "        # Prepare data consistency blocks\n",
        "        self.dc = nn.ModuleList([DCPM(A, AH, weight_init=1.0, max_iter=10) for _ in range(self.T)])\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        x = inputs[0]  # undersampled image\n",
        "        for i in range(self.T_end):  # unrolled network\n",
        "            ii = i % self.T\n",
        "            # TODO: Forward path\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "model = UnrolledNetwork(cascades=5)\n",
        "\n",
        "# Example forward path\n",
        "inputs, _ = next(iter(training_generator))  # Get a batch from the training generator\n",
        "inputs = [inp.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) for inp in inputs]\n",
        "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "output = model(inputs)\n",
        "\n",
        "# Print model summary\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training\n",
        "\n",
        "Before we can start our training we need to update our training loop to consider the additional network inputs and outputs.\n",
        "\n",
        "**Task 36: Write an updated training loop. Use the previous training loop from Task 15 as starting point.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Define the training loop\n",
        "def train_model_rawdata(model, train_loader, val_loader, criterion, optimizer, epochs, mse_metric=mse_metric, mae_metric=mae_metric):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        train_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        with tqdm(total=len(train_loader), desc=\"Training\", unit=\"batch\") as pbar:\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs = # TODO: input handling\n",
        "                # TODO: implement the training path\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                pbar.update(1)  # Update progress bar\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        val_mse = 0.0\n",
        "        val_mae = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs = # TODO: input handling\n",
        "                targets = targets.to(device)\n",
        "                # TODO: implement the validation path\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_mse += mse_metric(outputs, targets)\n",
        "                val_mae += mae_metric(outputs, targets)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
        "              f\"Validation Loss: {val_loss / len(val_loader):.4f}, \"\n",
        "              f\"Validation MSE: {val_mse / len(val_loader):.4f}, \"\n",
        "              f\"Validation MAE: {val_mae / len(val_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Task 37: Assign an Adam optimizer (learning rate 0.001) with complex-valued MSE loss function. Use the MSE and MAE for validation metrics. Train for 3 epochs.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from merlinth.losses import mse, mae\n",
        "\n",
        "criterion = # TODO: Define the loss function\n",
        "optimizer = # TODO: Adam optimizer with learning rate 0.001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)  # Move model to device\n",
        "train_model_rawdata(model, training_generator, validation_generator, criterion, optimizer, epochs=3, mse_metric=mse, mae_metric=mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test\n",
        "\n",
        "**Task 38: Update the test loop from Task 16 to consider the input list.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained model\n",
        "def evaluate_model_rawdata(model, test_loader, criterion, mse_metric=mse_metric, mae_metric=mae_metric):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    test_mse = 0.0\n",
        "    test_mae = 0.0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs = # TODO: input handling\n",
        "            targets = targets.to(device)\n",
        "            # TODO: implement the evaluation path\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            test_mse += mse_metric(outputs, targets)\n",
        "            test_mae += mae_metric(outputs, targets)\n",
        "\n",
        "    # Compute average metrics\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "    avg_mse = test_mse / len(test_loader)\n",
        "    avg_mae = test_mae / len(test_loader)\n",
        "\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test MSE: {avg_mse:.4f}, Test MAE: {avg_mae:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Task 39: Test the trained model to predict a denoised output and to display performance (metrics) on the test set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from merlinth.losses import mse, mae\n",
        "# evaluate trained model\n",
        "evaluate_model_rawdata(model, test_generator, criterion, mse_metric=mse, mae_metric=mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict with trained model (Torch version)\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    inputs, outputs =  # TODO: Get a batch from the test generator\n",
        "    inputs = [inp.to(device) for inp in inputs]  # Move inputs to the appropriate device\n",
        "    outputs = outputs.to(device)  # Move outputs to the appropriate device\n",
        "    predicted_output = model(inputs)  # TODO: Forward path\n",
        "\n",
        "# Display the predicted output\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "icase = 0  # Display the first example\n",
        "plt.figure()\n",
        "\n",
        "# Magnitude - Noisy\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(np.abs(inputs[0][icase, 0].cpu().numpy()), cmap='gray')\n",
        "plt.title('Magnitude - Noisy')\n",
        "plt.axis('off')\n",
        "\n",
        "# Magnitude - Recon\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.imshow(np.abs(predicted_output[icase, 0].cpu().numpy()), cmap='gray')\n",
        "plt.title('Magnitude - Recon')\n",
        "plt.axis('off')\n",
        "\n",
        "# Magnitude - Target\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.imshow(np.abs(outputs[icase, 0].cpu().numpy()), cmap='gray')\n",
        "plt.title('Magnitude - Target')\n",
        "plt.axis('off')\n",
        "\n",
        "# Phase - Noisy\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.imshow(np.angle(inputs[0][icase, 0].cpu().numpy()), vmin=-np.pi, vmax=np.pi, cmap='hsv')\n",
        "plt.title('Phase - Noisy')\n",
        "plt.axis('off')\n",
        "\n",
        "# Phase - Recon\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.imshow(np.angle(predicted_output[icase, 0].cpu().numpy()), vmin=-np.pi, vmax=np.pi, cmap='hsv')\n",
        "plt.title('Phase - Recon')\n",
        "plt.axis('off')\n",
        "\n",
        "# Phase - Target\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.imshow(np.angle(outputs[icase, 0].cpu().numpy()), vmin=-np.pi, vmax=np.pi, cmap='hsv')\n",
        "plt.title('Phase - Target')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIHKhUP9vwNc"
      },
      "source": [
        "# Further hands-on for Machine Learning reconstruction\n",
        "\n",
        "You might want to check out further details for machine learning image reconstruction. We have compiled a few Jupyter notebooks to let you explore the image of complex-valued vs. real-valued processing and thec choice of the respective layers for that:\n",
        "- [Image denoising (magnitude)](https://github.com/midas-tum/merlin/blob/master/notebooks/tutorial_denoising_real.ipynb)\n",
        "- [Image denoising (2-channel real-valued)](https://github.com/midas-tum/merlin/blob/master/notebooks/tutorial_denoising_2chreal.ipynb)\n",
        "- [Image denoising (complex-valued)](https://github.com/midas-tum/merlin/blob/master/notebooks/tutorial_denoising_complex.ipynb)\n",
        "- [Image reconstruction (complex-valued)](https://github.com/midas-tum/merlin/blob/master/notebooks/tutorial_reconstruction_complex.ipynb)\n",
        "- [Plug and Play Prior reconstruction](https://github.com/ISMRM-MIT-CMR/CMR-DL-challenge)\n",
        "- [Complex-valued activation functions](https://github.com/midas-tum/merlin/blob/master/notebooks/tutorial_complex_activations.ipynb)\n",
        "- [Complex layers](https://github.com/midas-tum/merlin/blob/master/notebooks/tutorial_complex_layers.ipynb)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Workbook_MRI_reconstruction.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ismrm25",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
